{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "devoicing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlNqGvC6KQrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "c0a03e1b-9d29-4937-93b4-d26e546fda1b"
      },
      "source": [
        "!pip install librosa==0.7.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting librosa==0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/b5/1817862d64a7c231afd15419d8418ae1f000742cac275e85c74b219cbccb/librosa-0.7.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (2.1.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.15.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (4.4.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (1.12.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.48.0)\n",
            "Collecting soundfile>=0.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.7.2) (47.3.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.2-cp36-none-any.whl size=1612885 sha256=ecf5c8fbf28022b1859330e8b6b8691cd6247f2d82a718123128000f33813441\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/6e/d7/bb93911540d2d1e44d690a1561871e5b6af82b69e80938abef\n",
            "Successfully built librosa\n",
            "Installing collected packages: soundfile, librosa\n",
            "  Found existing installation: librosa 0.6.3\n",
            "    Uninstalling librosa-0.6.3:\n",
            "      Successfully uninstalled librosa-0.6.3\n",
            "Successfully installed librosa-0.7.2 soundfile-0.10.3.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBwMmp6s8DNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, \\\n",
        "  Conv2D, Conv2DTranspose, UpSampling2D, \\\n",
        "  LeakyReLU, ZeroPadding2D, Cropping2D, MaxPooling2D, \\\n",
        "  BatchNormalization, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import librosa.feature\n",
        "import librosa.output\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y91Sug1KVcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f2e89731-cf7f-4739-b826-58edbb096f05"
      },
      "source": [
        "drive.mount('/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yCHp2AeLIQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "4749e0f6-7492-4a3b-f7cf-403274bab06a"
      },
      "source": [
        "!ls '/gdrive/My Drive/bosch/processed/pop' | head -10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00Mb3DuaIH1kjrwOku9CGU.npy\n",
            "02tvc9CFnTyHuSRlGeNv9w.npy\n",
            "03mMSLEJCPoGJwQhHpN5y0.npy\n",
            "04ZTP5KsCypmtCmQg5tH9R.npy\n",
            "06scTb0zbkxYNgpAB3J9fN.npy\n",
            "07Oz5StQ7GRoygNLaXs2pd.npy\n",
            "08bNPGLD8AhKpnnERrAc6G.npy\n",
            "08tq1XcHwVt5PHXUo87i0D.npy\n",
            "09PGubKAMryhOWv1LHpCYz.npy\n",
            "0afhq8XCExXpqazXczTSve.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRfGpQ0bLrsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "3b4bcda7-47c9-4b8c-945c-da2d148423bd"
      },
      "source": [
        "!ls '/gdrive/My Drive/bosch/processed-instr/pop' | head -10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00Mb3DuaIH1kjrwOku9CGU.npy\n",
            "02tvc9CFnTyHuSRlGeNv9w.npy\n",
            "03mMSLEJCPoGJwQhHpN5y0.npy\n",
            "04ZTP5KsCypmtCmQg5tH9R.npy\n",
            "06scTb0zbkxYNgpAB3J9fN.npy\n",
            "07Oz5StQ7GRoygNLaXs2pd.npy\n",
            "08bNPGLD8AhKpnnERrAc6G.npy\n",
            "08tq1XcHwVt5PHXUo87i0D.npy\n",
            "09PGubKAMryhOWv1LHpCYz.npy\n",
            "0afhq8XCExXpqazXczTSve.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GlCmHMGLuAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "H = 128\n",
        "W = 1290\n",
        "C = 1\n",
        "SAMPLES = 15\n",
        "SR = 22050\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Reference values to normalize data\n",
        "min_level_db=-100\n",
        "ref_level_db=20\n",
        "\n",
        "ROOT_DIR = '/gdrive/My Drive/bosch/'\n",
        "WEIGHTS_PATH = '/gdrive/My Drive/bosch/devoicing'\n",
        "GENRES = ['pop', 'blues', 'rockabilly', 'hip-hop']\n",
        "\n",
        "assert W % SAMPLES == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdb3xuqIsg1v",
        "colab_type": "text"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvXqgtKbsfP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(S):\n",
        "  return np.clip((((S - min_level_db) / -min_level_db)*2.) - 1., -1, 1)\n",
        "\n",
        "def denormalize(S):\n",
        "  return (((np.clip(S, -1, 1) + 1.) / 2.) * -min_level_db) + min_level_db\n",
        "\n",
        "def prep(S: np.array):\n",
        "  S_db = librosa.power_to_db(S) - ref_level_db\n",
        "  return normalize(S_db)\n",
        "\n",
        "# def deprep(S):\n",
        "#   S = denormalize(S) + ref_level_db\n",
        "#   S = librosa.db_to_power(S)\n",
        "#   wv = GRAD(np.expand_dims(S,0), melspecfunc, maxiter=2000, evaiter=10, tol=1e-8)\n",
        "#   return np.array(np.squeeze(wv))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rD8rikXY4Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = [\n",
        "  (f'{ROOT_DIR}/processed/{gen}/{full_fn}', f'{ROOT_DIR}/processed-instr/{gen}/{instr_fn}') \n",
        "  for gen in GENRES \n",
        "  for (full_fn, instr_fn) in zip(os.listdir(f'{ROOT_DIR}/processed/{gen}')[:100], os.listdir(f'{ROOT_DIR}/processed/{gen}')[:100])\n",
        "]\n",
        "\n",
        "# for fns in filenames:\n",
        "#   print(fns)\n",
        "\n",
        "dataset = [\n",
        "  (np.expand_dims(np.load(full_fn)[:, :1290], axis=2), np.expand_dims(np.load(instr_fn)[:, :1290], axis=2))\n",
        "  for (full_fn, instr_fn) in filenames\n",
        "]\n",
        "\n",
        "dataset = [\n",
        "  (s_full, s_instr) for row in dataset \n",
        "  for (s_full, s_instr) in zip(np.split(row[0], SAMPLES, axis=1), np.split(row[1], SAMPLES, axis=1))\n",
        "]\n",
        "\n",
        "dataset = [(prep(s_full), prep(s_instr)) for (s_full, s_instr) in dataset]\n",
        "\n",
        "assert dataset[0][0].shape == (128, W // SAMPLES, 1)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dataset).repeat(20).shuffle(100000).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcvdzXhz_6B9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "49f220bd-f69c-43c9-c9ae-b34f3d347317"
      },
      "source": [
        "NUM_BATCHES = tf.data.experimental.cardinality(dataset).numpy()\n",
        "\n",
        "train_batches = 0.8 * NUM_BATCHES\n",
        "train_ds = dataset.take(train_batches)\n",
        "\n",
        "valid_test_ds = dataset.skip(train_batches)\n",
        "valid_batches = 0.1 * NUM_BATCHES\n",
        "valid_ds = valid_test_ds.take(valid_batches)\n",
        "test_ds = valid_test_ds.skip(valid_batches)\n",
        "\n",
        "print('TOTAL', NUM_BATCHES)\n",
        "print('VALID', tf.data.experimental.cardinality(valid_ds).numpy())\n",
        "print('TEST', tf.data.experimental.cardinality(test_ds).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOTAL 1875\n",
            "VALID 187\n",
            "TEST 188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Azbk5-tpt0",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT7cVd4QpjSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.keras.initializers.he_uniform()\n",
        "\n",
        "def conv2d(layer_input, filters, kernel_size=4, strides=2, padding='same', leaky=True, bnorm=True):\n",
        "  if leaky:\n",
        "    Activ = LeakyReLU(alpha=0.2)\n",
        "  else:\n",
        "    Activ = ReLU()\n",
        "  d = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
        "  if bnorm:\n",
        "    d = BatchNormalization()(d)\n",
        "  d = Activ(d)\n",
        "  return d\n",
        "\n",
        "def deconv2d(layer_input, layer_res, filters, kernel_size=4, conc=True, scalev=False, bnorm=True, up=True, padding='same', strides=2):\n",
        "  if up:\n",
        "    u = UpSampling2D((1,2))(layer_input)\n",
        "    u = Conv2D(filters, kernel_size, strides=(1,1), kernel_initializer=init, use_bias=False, padding=padding)(u)\n",
        "  else:\n",
        "    u = Conv2DTranspose(filters, kernel_size, strides=strides, kernel_initializer=init, use_bias=False, padding=padding)(layer_input)\n",
        "  if bnorm:\n",
        "    u = BatchNormalization()(u)\n",
        "  u = LeakyReLU(alpha=0.2)(u)\n",
        "  if conc:\n",
        "    u = Concatenate()([u,layer_res])\n",
        "  return u"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA3yK1D3KmCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#U-NET style architecture\n",
        "def build_model(input_shape):\n",
        "  h, w, c = input_shape\n",
        "  inp = Input(shape=input_shape)\n",
        "  #downscaling\n",
        "  g0 = tf.keras.layers.ZeroPadding2D((0,1))(inp)\n",
        "  print('G0', g0.shape)\n",
        "  g1 = conv2d(g0, 256, kernel_size=(h,3), strides=1, padding='valid')\n",
        "  print('G1', g1.shape)\n",
        "  g1_p = tf.keras.layers.ZeroPadding2D((0,1))(g1)\n",
        "  g2 = conv2d(g1_p, 256, kernel_size=(1,9), strides=(1,2))\n",
        "  print('G2', g2.shape)\n",
        "  g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  print('G3', g3.shape)\n",
        "  #upscaling\n",
        "  g4 = deconv2d(g3, g2, 256, kernel_size=(1,7), strides=(1,2))\n",
        "  print('G4', g4.shape)\n",
        "  g5 = deconv2d(g4, tf.keras.layers.ZeroPadding2D((0,1))(g1), 256, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
        "  print('G5', g5.shape)\n",
        "  g6 = Conv2DTranspose(1, kernel_size=(h,1), strides=(1,1), activation='tanh', kernel_initializer=init, padding='valid')(g5)\n",
        "  print('G6', g6.shape)\n",
        "  g7 = tf.keras.layers.Cropping2D(cropping=((0, 0), (1, 1)))(g6)\n",
        "  print('G7', g7.shape)\n",
        "  return Model(inp, g7, name='G')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYvQ6WqcRDDb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa59a82c-654d-4a94-b77f-e0a55267416a"
      },
      "source": [
        "LOAD_WEIGHTS = True\n",
        "\n",
        "print(W // SAMPLES)\n",
        "model = build_model((H, W // SAMPLES, C))\n",
        "\n",
        "if LOAD_WEIGHTS == True and len(glob.glob(f'{WEIGHTS_PATH}/*.h5')) != 0:\n",
        "  print('LOADING WEIGHTS..')\n",
        "  model.load_weights(f'{WEIGHTS_PATH}/model.h5')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86\n",
            "G0 (None, 128, 88, 1)\n",
            "G1 (None, 1, 86, 256)\n",
            "G2 (None, 1, 44, 256)\n",
            "G3 (None, 1, 22, 256)\n",
            "G4 (None, 1, 44, 512)\n",
            "G5 (None, 1, 88, 512)\n",
            "G6 (None, 128, 88, 1)\n",
            "G7 (None, 128, 86, 1)\n",
            "LOADING WEIGHTS..\n",
            "Model: \"G\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128, 86, 1)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 128, 88, 1)   0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 1, 86, 256)   98304       zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 1, 86, 256)   1024        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 1, 86, 256)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 1, 88, 256)   0           leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 1, 44, 256)   589824      zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 1, 44, 256)   1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 1, 44, 256)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 1, 22, 256)   458752      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 1, 22, 256)   1024        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 1, 22, 256)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 1, 44, 256)   0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 1, 44, 256)   458752      up_sampling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 1, 44, 256)   1024        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 1, 44, 256)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1, 44, 512)   0           leaky_re_lu_3[0][0]              \n",
            "                                                                 leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 1, 88, 512)   0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 1, 88, 256)   1179648     up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 1, 88, 256)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_2 (ZeroPadding2D (None, 1, 88, 256)   0           leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1, 88, 512)   0           leaky_re_lu_4[0][0]              \n",
            "                                                                 zero_padding2d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 128, 88, 1)   65537       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d (Cropping2D)         (None, 128, 86, 1)   0           conv2d_transpose[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 2,854,913\n",
            "Trainable params: 2,852,865\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImFW055Y8jBY",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_-sVSzJ8T39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(x, y):\n",
        "  \"\"\"Calculate L2 error between given vectors.\"\"\"\n",
        "  return tf.reduce_mean((x-y)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlNNBzEg8k9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "LR = 0.0002\n",
        "REPORT_STEP = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_qSAoV788eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(LR, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiV7Gu1sTSxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_loss = []\n",
        "# valid_loss = []\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#   start_time = time.time()\n",
        "#   epoch_loss = []  # Use it for collecting training losses\n",
        "\n",
        "#   # Training\n",
        "#   for batch_idx, batch in enumerate(train_ds):\n",
        "\n",
        "#     with tf.GradientTape() as tape:\n",
        "#       devocal_tensor = model(batch[:, 0, :, :, :])  # Input the full spectograms\n",
        "#       loss = mse(devocal_tensor, batch[:, 1, :, :, :])  # Compare output with instrumental spectograms\n",
        "#       gradient = tape.gradient(loss, model.trainable_variables)\n",
        "#       optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
        "    \n",
        "#     epoch_loss.append((epoch, loss))\n",
        "#     if (batch_idx + 1) % REPORT_STEP == 0:\n",
        "#       print(f'[Epoch {epoch}/{EPOCHS}] [Batch {batch_idx+1}/ {NUM_BATCHES}] ', end='')\n",
        "#       print(f'[Loss {np.mean(epoch_loss[-REPORT_STEP:])}]', end='')\n",
        "#       print(f' Time/Batch {(time.time()-start_time)/(batch_idx + 1)}')\n",
        "#       print('WEIGHTS SAVED..')\n",
        "#       model.save_weights(f'{WEIGHTS_PATH}/model.h5')\n",
        "\n",
        "#   train_loss.append(np.mean(epoch_loss))\n",
        "\n",
        "#   # Validation\n",
        "#   epoch_loss = []  # Reuse the array for collecting validation losses\n",
        "#   for batch_idx, batch in enumerate(valid_ds):\n",
        "\n",
        "#     full_spec = batch[:, 0, :, :, :]\n",
        "#     instr_spec = batch[:, 1, :, :, :]\n",
        "\n",
        "#     devocal_tensor = model(full_spec)\n",
        "#     loss = mse(devocal_tensor, instr_spec)\n",
        "#     epoch_loss.append(loss)\n",
        "\n",
        "#   clear_output(wait=True)\n",
        "#   valid_loss.append(np.mean(epoch_loss))\n",
        "#   for loss in valid_loss:\n",
        "#     print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}